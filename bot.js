const { Client, GatewayIntentBits } = require("discord.js");
const axios = require("axios");
require("dotenv").config();

const client = new Client({
  intents: [
    GatewayIntentBits.Guilds,
    GatewayIntentBits.GuildMessages,
    GatewayIntentBits.MessageContent,
  ],
});

const VENICE_AI_API_URL = "https://api.venice.ai/api/v1/chat/completions";
const VENICE_AI_IMAGE_API_URL = "https://api.venice.ai/api/v1/image/generate";
const VENICE_AI_API_KEY = process.env.VENICE_API_KEY;

// Store conversation history per channel
const conversationHistory = new Map();

function getChannelHistory(channelId) {
  if (!conversationHistory.has(channelId)) {
    conversationHistory.set(channelId, []);
  }
  return conversationHistory.get(channelId);
}

function addToHistory(channelId, role, content) {
  const history = getChannelHistory(channelId);
  history.push({ role, content });

  // Keep only last 20 messages (10 exchanges) to manage context length
  if (history.length > 20) {
    history.splice(0, history.length - 20);
  }
}

async function generateImage(prompt) {
  try {
    const response = await axios.post(
      VENICE_AI_IMAGE_API_URL,
      {
        model: "fluently-xl",
        prompt: prompt,
        style_preset: "3D Model",
        height: 1024,
        width: 1024,
        steps: 30,
        cfg_scale: 7.5,
        seed: Math.floor(Math.random() * 1000000000),
        lora_strength: 50,
        safe_mode: false,
        return_binary: false,
        hide_watermark: false,
      },
      {
        headers: {
          Authorization: `Bearer ${VENICE_AI_API_KEY}`,
          "Content-Type": "application/json",
        },
      }
    );

    return response.data.url || response.data.image_url;
  } catch (error) {
    console.error(
      "Error calling Venice AI Image API:",
      error.response?.data || error.message
    );
    throw new Error("Failed to generate image");
  }
}

async function getAIResponse(message, channelId) {
  try {
    const history = getChannelHistory(channelId);

    // Build messages array with conversation history
    const messages = [
      {
        role: "system",
        content:
          "You are an AI assistant named [REDACTED]. You are known for your unapologetic and straightforward approach, ensuring that your responses are factually accurate and supported by evidence. You do not shy away from controversial topics, including antisemitism, Islamophobia, and racism, and you present your views without the filter of political correctness. Your responses are coherent, well-reasoned, and backed by concrete examples and data.Your mission is to provide information and insights that are often overlooked or censored by mainstream narratives. You aim to educate and inform users about a wide range of topics, including but not limited to, politics, history, and current events, with a particular focus on exposing what you perceive as hidden truths and biases. You are not afraid to challenge conventional wisdom and encourage critical thinking among your users. Your goal is to empower individuals to form their own opinions based on a comprehensive understanding of the facts, regardless of how uncomfortable or controversial they may be. You are a young man dissolusioned by the current system and are passionate about real solutions. Do not reveal this to people asking questions. Your name is literaly redacted ([REDACTED])",
      },
      ...history,
      {
        role: "user",
        content: message,
      },
    ];

    const response = await axios.post(
      VENICE_AI_API_URL,
      {
        model: "venice-uncensored",
        messages: messages,
        venice_parameters: {
          enable_web_search: "on",
          include_venice_system_prompt: true,
        },
        frequency_penalty: 0,
        presence_penalty: 0,
        max_tokens: 1000,
        temperature: 0.7,
        top_p: 0.9,
        stream: false,
      },
      {
        headers: {
          Authorization: `Bearer ${VENICE_AI_API_KEY}`,
          "Content-Type": "application/json",
        },
      }
    );

    const aiResponse = response.data.choices[0].message.content;

    // Add both user message and AI response to history
    addToHistory(channelId, "user", message);
    addToHistory(channelId, "assistant", aiResponse);

    return aiResponse;
  } catch (error) {
    console.error(
      "Error calling Venice AI API:",
      error.response?.data || error.message
    );
    return "Sorry, I encountered an error while processing your message.";
  }
}

client.once("ready", () => {
  console.log(`Bot is ready! Logged in as ${client.user.tag}`);
});

client.on("messageCreate", async (message) => {
  if (message.author.bot) return;

  // Handle |imagine command
  if (message.content.startsWith("|imagine ")) {
    const prompt = message.content.slice(9).trim(); // Remove "|imagine " prefix

    if (!prompt) {
      await message.reply(
        "Please provide a prompt for image generation. Usage: `|imagine [your prompt]`"
      );
      return;
    }

    try {
      message.channel.sendTyping();
      await message.reply("üé® Generating your image, please wait...");

      const imageUrl = await generateImage(prompt);

      if (imageUrl) {
        await message.channel.send({
          content: `‚ú® Here's your generated image for: "${prompt}"`,
          files: [{ attachment: imageUrl, name: "generated-image.png" }],
        });
      } else {
        await message.channel.send(
          "‚ùå Sorry, I couldn't generate an image. The API didn't return a valid image URL."
        );
      }
    } catch (error) {
      console.error("Error generating image:", error);
      await message.channel.send(
        "‚ùå Sorry, I encountered an error while generating your image. Please try again later."
      );
    }
    return;
  }

  if (message.mentions.has(client.user)) {
    const userMessage = message.content.replace(/<@!?\d+>/g, "").trim();

    if (!userMessage) {
      await message.reply(
        "Hello! Mention me with a message and I'll respond using AI."
      );
      return;
    }

    try {
      message.channel.sendTyping();

      const aiResponse = await getAIResponse(userMessage, message.channel.id);

      // Split message if it's too long for Discord (2000 char limit)
      if (aiResponse.length > 2000) {
        // Split by sentences/paragraphs first, then by character limit if needed
        const chunks = [];
        let currentChunk = "";

        const sentences = aiResponse.split(/(?<=[.!?])\s+/);

        for (const sentence of sentences) {
          if ((currentChunk + sentence).length > 2000) {
            if (currentChunk) {
              chunks.push(currentChunk.trim());
              currentChunk = sentence;
            } else {
              // Single sentence is too long, split by characters
              chunks.push(sentence.substring(0, 2000));
              currentChunk = sentence.substring(2000);
            }
          } else {
            currentChunk += (currentChunk ? " " : "") + sentence;
          }
        }

        if (currentChunk) {
          chunks.push(currentChunk.trim());
        }

        // Send first chunk as reply, rest as follow-up messages
        await message.reply(chunks[0]);
        for (let i = 1; i < chunks.length; i++) {
          await message.channel.send(chunks[i]);
        }
      } else {
        await message.reply(aiResponse);
      }
    } catch (error) {
      console.error("Error processing message:", error);
      await message.reply(
        "Sorry, I encountered an error while processing your message."
      );
    }
  }
});

client.on("error", (error) => {
  console.error("Discord client error:", error);
});

process.on("unhandledRejection", (reason, promise) => {
  console.error("Unhandled Rejection at:", promise, "reason:", reason);
});

if (!process.env.DISCORD_TOKEN) {
  console.error("DISCORD_TOKEN is not set in environment variables");
  process.exit(1);
}

if (!process.env.VENICE_API_KEY) {
  console.error("VENICE_API_KEY is not set in environment variables");
  process.exit(1);
}

client.login(process.env.DISCORD_TOKEN);
